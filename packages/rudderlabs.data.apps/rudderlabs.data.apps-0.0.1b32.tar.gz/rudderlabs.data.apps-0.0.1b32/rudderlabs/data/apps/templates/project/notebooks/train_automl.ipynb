{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import joblib\n",
    "import json\n",
    "import logging\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import display\n",
    "from pprint import pprint\n",
    "from typing import Dict, List, Tuple, Union\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "#Rudderlab data utilities imports\n",
    "from rudderlabs.data.apps.log import setup_file_logger\n",
    "from rudderlabs.data.apps.config import read_yaml\n",
    "from rudderlabs.data.apps.utils import get_latest_folder\n",
    "from rudderlabs.data.apps.aws.s3 import upload_file_to_s3\n",
    "\n",
    "from sklearn.metrics import average_precision_score, precision_recall_fscore_support, roc_auc_score, f1_score\n",
    "from sklearn.metrics import precision_score, recall_score, precision_recall_curve, roc_curve\n",
    "from sklearn.metrics import get_scorer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "\n",
    "pd.options.display.max_columns=None\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters cell for papermill. These values can get overridden by parameters passed by papermill\n",
    "job_id = str(int(time.time()))\n",
    "local_input_path = None\n",
    "local_output_path = None\n",
    "code_path = \"../\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize input and output paths if they are not passed by papermill\n",
    "if local_input_path is None:\n",
    "    local_input_path = f\"../data/{job_id}/train_automl\"\n",
    "    \n",
    "if local_output_path is None:\n",
    "    local_output_path = f\"../data/{job_id}/train_automl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(job_id)\n",
    "print(f\"local_input_path {local_input_path}\")\n",
    "print(f\"local_output_path {local_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Local imports\n",
    "sys.path.append(code_path)\n",
    "from data_loader import DataIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "# All the required constants are defined here\n",
    "IMAGE_FORMAT = 'png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logging setup\n",
    "try:\n",
    "    log_file_path = os.path.join(local_output_path, \"logs\", \"sample_notebook.log\")\n",
    "    logging = setup_file_logger(log_file_path)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "logging.info(\"\\n\\n\\t\\tSTARTING FEATURE PREPROCESSING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configurations\n",
    "notebook_config = read_yaml(os.path.join(code_path, \"config/train_automl.yaml\"))\n",
    "print(\"Notebook config:\")\n",
    "pprint(notebook_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creds_config = read_yaml(os.path.join(code_path, \"credentials.yaml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the output files get stored in the output_directory. Each run of the feature_processing generates a new sub directory based on the timestamp.\n",
    "# output directory structure\n",
    "# - data\n",
    "#   - <job_id>\n",
    "#       - data-prep\n",
    "#           - visuals\n",
    "#           - model_artifacts\n",
    "visuals_dir = os.path.join( local_output_path, \"visuals\" )\n",
    "model_artifacts_dir = os.path.join(local_output_path, \"model_artifacts\")\n",
    "\n",
    "logging.info(f\"All the output files will be saved to following location: {local_output_path}\")\n",
    "for output_path in [local_output_path, visuals_dir, model_artifacts_dir]:\n",
    "    Path(output_path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data splitting\n",
    "train_split = data_prep_config['data']['train_size']\n",
    "val_split = notebook_config['data']['val_size']\n",
    "test_split = notebook_config['data']['test_size']\n",
    "\n",
    "ignore_features = notebook_config['data']['ignore_features']\n",
    "label_column = notebook_config['data']['label_column']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Getting data from warehouse\")\n",
    "dataIO = DataIO(notebook_config, creds_config)\n",
    "input_data = dataIO.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ignoring features\n",
    "#Select valid columns to ignore from the feature table\n",
    "ignore_features = [ col for col in ignore_features if col in input_data.columns ]\n",
    "print(f\"Ignoring features {ignore_features}\")\n",
    "logging.info(f\"Ignoring features {ignore_features}\")\n",
    "input_data = input_data.drop(columns=ignore_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Basic stats of all numerical features in input:\")\n",
    "pd.options.display.max_columns = None\n",
    "# Histograms for each numeric features\n",
    "display(input_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(input_data, test_size=val_split+test_split)\n",
    "X_val, X_test = train_test_split(X_test, test_size=test_split/(test_split + val_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = X_train[label_column]\n",
    "y_test = X_test[label_column]\n",
    "y_val = X_val[label_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_metrics(y_true, y_pred_proba, th=0.5):\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, np.where(y_pred_proba>th,1,0))\n",
    "    precision = precision[1]\n",
    "    recall = recall[1]\n",
    "    f1 = f1[1]\n",
    "    roc_auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    pr_auc = average_precision_score(y_true, y_pred_proba)\n",
    "    metrics = {\"precision\": precision, \"recall\": recall, \"f1_score\": f1, \"roc_auc\": roc_auc, 'pr_auc': pr_auc}\n",
    "    return metrics\n",
    "\n",
    "def get_best_th(y_true, y_pred_proba):\n",
    "    \"\"\"\n",
    "    Returns the threshold that maximizes f1 score based on y_true and y_pred_proba\n",
    "    Also returns the metrics at the threshold\n",
    "    y_true: Array of 1s and 0s. True labels\n",
    "    y_pred_proba: Array of predicted probabilities\n",
    "    \"\"\"\n",
    "    best_f1 = 0.0\n",
    "    best_th = 0.0\n",
    "   \n",
    "    for th in np.arange(0,1,0.01):\n",
    "        f1 = f1_score(y_true, np.where(y_pred_proba>th,1,0))\n",
    "        if f1 >= best_f1:\n",
    "            best_th = th\n",
    "            best_f1 = f1\n",
    "            \n",
    "    best_metrics = get_classification_metrics(y_true, y_pred_proba, best_th)\n",
    "    return best_metrics, best_th"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pycaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.classification import *\n",
    "import pycaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_pycaret(\n",
    "    best_model,\n",
    "    X_train: pd.DataFrame, y_train: pd.DataFrame,\n",
    "    X_test: pd.DataFrame, y_test: pd.DataFrame,\n",
    "    X_val: pd.DataFrame, y_val: pd.DataFrame\n",
    "):\n",
    "    train_preds = pd.DataFrame(predict_model(best_model, X_train, raw_score=True))[\"prediction_score_1\"]\n",
    "    train_metrics, prob_threshold = get_best_th(y_train, train_preds)   \n",
    "\n",
    "    test_preds = pd.DataFrame(predict_model(best_model, X_test, raw_score=True))[\"prediction_score_1\"]\n",
    "    test_metrics = get_classification_metrics(y_test, test_preds, prob_threshold)\n",
    "\n",
    "    val_preds = pd.DataFrame(predict_model(best_model, X_val, raw_score=True))[\"prediction_score_1\"]\n",
    "    val_metrics = get_classification_metrics(y_val, val_preds, prob_threshold)\n",
    "\n",
    "    metrics = {\"train\": train_metrics, \"val\": val_metrics, \"test\": test_metrics}\n",
    "    predictions = {\"train\": train_preds, \"val\": val_preds, \"test\": test_preds}\n",
    "    \n",
    "    return metrics, predictions, prob_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "experiment = setup(data = X_train, target = label_column ,session_id=1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "best_model = compare_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pull()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(best_model, plot = 'auc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tunned_model = tune_model(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_evaluations_results = {}\n",
    "metrics, predictions, prob_threshold = get_metrics_pycaret(tunned_model, X_train, y_train, X_test, y_test, X_val, y_val)\n",
    "models_evaluations_results = {\n",
    "        \"metrics\" : metrics,\n",
    "        \"predictions\" : predictions,\n",
    "        \"prob_threshold\" : prob_threshold\n",
    "    }\n",
    "model_name = results.Model.tolist()[0]\n",
    "print(f\"\\n{model_name}\")\n",
    "results_df = pd.DataFrame.from_records(metrics).T.round(3)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "save_model(tunned_model, os.path.join(model_artifacts_dir, \"saved_model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actuals = {\"train\": y_train, \"test\": y_test, \"val\": y_val}\n",
    "y_preds = predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a379df",
   "metadata": {},
   "source": [
    "### Uploading Model files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b02d7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Uploading saved model file\")\n",
    "upload_file_to_s3(\n",
    "    creds = creds_config,\n",
    "    local_file_path = f\"{model_artifacts_dir}/saved_model.pkl\",\n",
    "    s3_bucket_name = creds_config[\"aws\"][\"s3Bucket\"],\n",
    "    s3_path = f\"{creds_config['aws']['staging_models_s3_prefix']}/{job_id}/saved_model.pkl\"\n",
    ")\n",
    "\n",
    "s3_location = f\"s3://{creds_config['aws']['s3Bucket']}/{creds_config['aws']['staging_models_s3_prefix']}/{job_id}\"\n",
    "print(f\" Model file is uploaded to:\\n\\t{s3_location}\")\n",
    "logging.info(f\" Model file is uploaded to:\\n\\t{s3_location}\")\n",
    "\n",
    "metrics_dict = {}\n",
    "\n",
    "for metric in notebook_config[\"model_params\"][\"evaluation_metrics\"]:\n",
    "    scorer = get_scorer(metric)\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        metrics_dict[f\"{split}_{metric}\"] = scorer._score_func(y_actuals[split], np.where(y_preds[split] >  prob_threshold,1,0))\n",
    "\n",
    "print(\"Adding entry to model registry\")\n",
    "data = {\n",
    "    \"job_id\": job_id,\n",
    "    \"model_name\" : \"leadscoring\",\n",
    "    \"model_type\" : \"staging\",\n",
    "    \"timestamp\" : datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"threshold\" : prob_threshold,\n",
    "    \"metrics\" : json.dumps(metrics_dict),\n",
    "    \"model_files_location\" : s3_location,\n",
    "    \"version\" : \"1.0.0\"\n",
    "}\n",
    "print(f\"Adding entry to model registry:\\n{data}\")\n",
    "logging.info(f\"Adding entry to model registry:\\n{data}\")\n",
    "\n",
    "data = pd.DataFrame(data, index=[0])\n",
    "data_io = DataIO(notebook_config, creds_config)\n",
    "\n",
    "data_io.write_to_wh_table(\n",
    "    df = data,\n",
    "    table_name = creds_config[\"data_warehouse\"][\"model_registry_table\"],\n",
    "    schema = creds_config[\"data_warehouse\"][\"schema\"],\n",
    "    if_exists = \"append\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d11067",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell to hide code while converting to a html page\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "$('div.input').hide();\n",
    "</script>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 ('ltv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee1f4f7cbbe0c25381182dcadec18cd65e36e6a61fec9f0f59adf335f73c8e1f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
