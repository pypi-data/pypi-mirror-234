{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04d2f8de",
   "metadata": {
    "tags": []
   },
   "source": [
    "## README:\n",
    "\n",
    "App : **Sample Application**\n",
    "\n",
    "Stage : **Data preparation**\n",
    "\n",
    "This is the sample notebook for loading data from warehouse\n",
    "\n",
    "The notebook expects the required inputs in the adjacent `data` folder\n",
    "\n",
    "Loading configuration from `config/data_prep.yaml` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ee3e37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import display\n",
    "from pprint import pprint\n",
    "\n",
    "#Rudderlab data utilities imports\n",
    "from rudderlabs.data.apps.log import setup_file_logger\n",
    "from rudderlabs.data.apps.config import read_yaml\n",
    "from rudderlabs.data.apps.utils.data import NamedColumns, get_onehot_encoder_names\n",
    "from rudderlabs.data.apps.aws.s3 import upload_file_to_s3\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import GenericUnivariateSelect, chi2, f_classif, VarianceThreshold\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "pd.options.display.max_columns=None\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0e3000",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters cell for papermill. These values can get overridden by parameters passed by papermill\n",
    "job_id = str(int(time.time()))\n",
    "local_input_path = None\n",
    "local_output_path = None\n",
    "code_path = \"../\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2b1743",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize input and output paths if they are not passed by papermill\n",
    "if local_input_path is None:\n",
    "    local_input_path = f\"../data/{job_id}/data-prep\"\n",
    "    \n",
    "if local_output_path is None:\n",
    "    local_output_path = f\"../data/{job_id}/data-prep\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2be85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(job_id)\n",
    "print(f\"local_input_path {local_input_path}\")\n",
    "print(f\"local_output_path {local_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e33bdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Local imports\n",
    "sys.path.append(code_path)\n",
    "from data_loader import DataIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e35075f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "# All the required constants are defined here\n",
    "IMAGE_FORMAT = 'png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7983552",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logging setup\n",
    "try:\n",
    "    log_file_path = os.path.join(local_output_path, \"logs\", \"sample_notebook.log\")\n",
    "    logging = setup_file_logger(log_file_path)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "logging.info(\"\\n\\n\\t\\tSTARTING FEATURE PREPROCESSING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20b1106",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Configurations\n",
    "notebook_config = read_yaml(os.path.join(code_path, \"config/data_prep.yaml\"))\n",
    "print(\"Notebook config:\")\n",
    "pprint(notebook_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb663a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "creds_config = read_yaml(os.path.join(code_path, \"credentials.yaml\"))\n",
    "print(\"Credentials config:\")\n",
    "pprint(creds_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825d861f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the output files get stored in the output_directory. Each run of the feature_processing generates a new sub directory based on the timestamp.\n",
    "# output directory structure\n",
    "# - data\n",
    "#   - <job_id>\n",
    "#       - data-prep\n",
    "#           - visuals\n",
    "#           - model_artifacts\n",
    "visuals_dir = os.path.join( local_output_path, \"visuals\" )\n",
    "model_artifacts_dir = os.path.join(local_output_path, \"model_artifacts\")\n",
    "\n",
    "logging.info(f\"All the output files will be saved to following location: {local_output_path}\")\n",
    "for output_path in [local_output_path, visuals_dir, model_artifacts_dir]:\n",
    "    Path(output_path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b544a1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data splitting\n",
    "train_split = notebook_config['data']['train_size']\n",
    "val_split = notebook_config['data']['val_size']\n",
    "test_split = notebook_config['data']['test_size']\n",
    "\n",
    "ignore_features = notebook_config['data']['ignore_features']\n",
    "label_column = notebook_config['data']['label_column']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6026bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Getting data from warehouse\")\n",
    "dataIO = DataIO(notebook_config, creds_config)\n",
    "input_data = dataIO.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439c5a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ignoring features\n",
    "#Select valid columns to ignore from the feature table\n",
    "ignore_features = [ col for col in ignore_features if col in input_data.columns ]\n",
    "print(f\"Ignoring features {ignore_features}\")\n",
    "logging.info(f\"Ignoring features {ignore_features}\")\n",
    "input_data = input_data.drop(columns=ignore_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26448f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sample rows from the transformed wide form data\")\n",
    "input_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e24d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2,figsize=(16,6))\n",
    "fig.suptitle(\"Label distribution\")\n",
    "input_data[label_column].value_counts().plot.pie(explode=[0,0.1], autopct=\"%1.1f%%\", ax=axs[0]);\n",
    "\n",
    "bars = (axs[1].barh(list(input_data[label_column].value_counts().index), list(input_data[label_column].value_counts().values)))\n",
    "\n",
    "for bars in axs[1].containers:\n",
    "    axs[1].bar_label(bars)\n",
    "    \n",
    "axs[1].set_yticks([0,1])\n",
    "axs[1].set_xlabel(\"Frequency\")\n",
    "axs[1].set_ylabel(\"Label\");\n",
    "\n",
    "plt.savefig(os.path.join(visuals_dir, f\"label_distribution.{IMAGE_FORMAT}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93e1d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Converting categorical columns to 'object' datatype \")\n",
    "categorical_columns = notebook_config['preprocessing']['categorical_columns']\n",
    "nl = \"\\n    - \"\n",
    "logging.info(f\"Categorical columns: {nl}{nl.join(categorical_columns)}\")\n",
    "print(f\"Categorical columns: {nl}{nl.join(categorical_columns)}\")\n",
    "\n",
    "input_data[categorical_columns] = input_data[categorical_columns].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc31fbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = list(input_data.select_dtypes(include='object'))\n",
    "numeric_columns = list(input_data.select_dtypes(exclude='object'))\n",
    "\n",
    "categorical_columns = [col for col in categorical_columns if col != label_column ]\n",
    "numeric_columns = [col for col in numeric_columns if col != label_column ]\n",
    "nl = \"\\n    - \"\n",
    "\n",
    "print(f\"Following are all categorical columns:{nl}{nl.join(categorical_columns)}\")\n",
    "print(f\"Numeric columns:{nl}{nl.join(numeric_columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8d2f10",
   "metadata": {},
   "source": [
    "preparing boolean columns to avoid any transformations, trying to find columns names where unique values are two among all numberic columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4395b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Detecting boolean columns\")\n",
    "logging.info(\"Detecting boolean columns\")\n",
    "boolean_columns = []\n",
    "\n",
    "for col in numeric_columns:\n",
    "    if len(input_data[col].unique()) == 2:\n",
    "        boolean_columns.append(col)\n",
    "\n",
    "print(f\"Boolean columns : {nl}{nl.join(boolean_columns)}\")\n",
    "logging.info(f\"Boolean columns : {nl}{nl.join(boolean_columns)}\")\n",
    "\n",
    "print(\"Removing boolean columns from numeric columns list\")\n",
    "for col in boolean_columns:\n",
    "    numeric_columns.remove(col)\n",
    "\n",
    "print(f\"Numeric columns:{nl}{nl.join(numeric_columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f76ad65",
   "metadata": {},
   "source": [
    "### Inspecting categorical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85124698",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Frequency tables for each categorical feature - Showing only top 5 categories\\n\")\n",
    "for column in input_data.select_dtypes(include=[\"object\"]).columns:\n",
    "    try:\n",
    "        print(f\"Feature: {column}\")\n",
    "        print(input_data[column].value_counts(normalize=True).round(4).head())\n",
    "        print(\"\\n\")\n",
    "    except TypeError:\n",
    "        print(f\"Unable to show cross tab for {column} variable. This typically happens if the variable type is unhashable. Ex: List. Sample values:\")\n",
    "        print(input_data.query(f\"~{column}.isnull()\", engine='python')[column].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3398ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Individual effect of each categorical variable on conversion - Showing only top 5 categories\\n\")\n",
    "for column in input_data.select_dtypes(include=[\"object\"]).columns:\n",
    "    if column == label_column:\n",
    "        continue\n",
    "    try:\n",
    "        display(pd.crosstab(input_data[column], input_data[label_column], normalize='index').round(4).head())\n",
    "        print(\"\\n\")\n",
    "    except TypeError:\n",
    "        print(f\"Unable to show cross tab for {column} variable. This typically happens if the variable type is unhashable. Ex: List. Sample values:\")\n",
    "        display(input_data.query(f\"~{column}.isnull()\", engine='python')[column].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87664930",
   "metadata": {},
   "source": [
    "### Inspecting numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85599b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Basic stats of all numerical features in input:\")\n",
    "pd.options.display.max_columns = None\n",
    "# Histograms for each numeric features\n",
    "display(input_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81098482",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Distribution of each numerical feature values:\")\n",
    "hist = input_data.hist(bins=30, sharey=True, figsize=(16, 16))\n",
    "plt.savefig(os.path.join(visuals_dir, f\"numerical_features_distribution.{IMAGE_FORMAT}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e75fb7",
   "metadata": {},
   "source": [
    "Let's plot the cluster map, where we can see correlation between pairs of features, and hierarchical clusters of the features. This helps in feature selection, to remove redundant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d06e7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "cor = input_data.corr()\n",
    "\n",
    "cluster_map = sns.clustermap(cor, cmap=sns.diverging_palette(20, 220, n=200), linewidths=0.1)\n",
    "plt.setp(cluster_map.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\n",
    "cluster_map\n",
    "plt.savefig(os.path.join(visuals_dir, f\"correlation.{IMAGE_FORMAT}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3cb72b",
   "metadata": {},
   "source": [
    "The above plot shows correlations between pairs. A correlation close to zero is ideal as that shows independent information. High correlation suggest that the features carry similar information and some of them may be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09b9f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Skewness in each numerical value:\")\n",
    "input_data.skew(axis=0, numeric_only=True).to_csv(os.path.join(visuals_dir, \"feature_skew.csv\"))\n",
    "input_data.skew(axis=0, numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191756ca",
   "metadata": {},
   "source": [
    "The above table shows skew of each numerical feature. A value close to zero indicates low skew. A large number either positive or negative indicates a large skew. For some algorithms, it might be useful to reduce the skew using transformations such as log transformation, power transformation etc.\n",
    "\n",
    "We would often want to normalize numerical features and apply transformations such as one-hot encoding on categorical features. The normalizer is taken as an input from the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507498b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = list(input_data.select_dtypes(exclude=[\"object\"]))\n",
    "categorical_columns = list(input_data.select_dtypes(include=[\"object\"]))\n",
    "\n",
    "categorical_columns = [col for col in categorical_columns if col != label_column ]\n",
    "numeric_columns = [col for col in numeric_columns if col != label_column ]\n",
    "\n",
    "print(f\"Following are all categorical columns:{nl}{nl.join(categorical_columns)}\")\n",
    "print(f\"Numeric columns:{nl}{nl.join(numeric_columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111f2c9f",
   "metadata": {},
   "source": [
    "### Train-Val-Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0442ff1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(input_data, test_size=val_split+test_split)\n",
    "X_val, X_test = train_test_split(X_test, test_size=test_split/(test_split + val_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9296805",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0451e373",
   "metadata": {},
   "source": [
    "### Column Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46d9488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare transforer name to instance dictionary for quering\n",
    "transformers =  { transformer.__name__: transformer for transformer in \n",
    "        [StandardScaler, PowerTransformer, MinMaxScaler, SimpleImputer, OneHotEncoder, FunctionTransformer]}\n",
    "feature_selectors = { selector.__name__: selector for selector in [GenericUnivariateSelect, chi2, f_classif, VarianceThreshold]}\n",
    "\n",
    "def build_pipeline(pipeline_config: dict) -> Pipeline:\n",
    "        pipeline_steps = []\n",
    "        for transform_options in pipeline_config:\n",
    "                options = transform_options.copy()\n",
    "                name = options.pop(\"name\")\n",
    "                #All other things will be treated as options to transformer function\n",
    "                pipeline_steps.append(\n",
    "                        (name, transformers[name](**options))\n",
    "                )\n",
    "\n",
    "        pipeline = Pipeline(steps=pipeline_steps)\n",
    "        return pipeline\n",
    "\n",
    "numeric_cols_to_transform = numeric_columns\n",
    "numeric_pipeline = build_pipeline(notebook_config[\"preprocessing\"][\"numeric_pipeline\"][\"pipeline\"])\n",
    "\n",
    "categorical_cols_to_transform = categorical_columns\n",
    "categorical_pipeline = build_pipeline(notebook_config[\"preprocessing\"][\"categorical_pipeline\"][\"pipeline\"])\n",
    "\n",
    "boolean_transformer = Pipeline(steps=[\n",
    "    ('identity', FunctionTransformer(lambda x:x))\n",
    "])\n",
    "\n",
    "preprocessing = ColumnTransformer(transformers=[\n",
    "        (\"num\", numeric_pipeline, numeric_cols_to_transform),\n",
    "        (\"cat\", categorical_pipeline, categorical_cols_to_transform),\n",
    "        (\"bool\", boolean_transformer, boolean_columns)\n",
    "], remainder=\"passthrough\")\n",
    "\n",
    "\n",
    "pipeline_steps = [\n",
    "        (\"reorder\", NamedColumns()),\n",
    "        (\"preprocessor\", preprocessing)\n",
    "]\n",
    "\n",
    "#Feature selection\n",
    "for i, feature_selection_options in enumerate(notebook_config[\"preprocessing\"][\"feature_selectors\"]):\n",
    "        options = feature_selection_options.copy()\n",
    "        name = options.pop(\"name\")\n",
    "        pipeline_steps.append(\n",
    "                (f\"feature_selector\", feature_selectors[name](**options))\n",
    "        )\n",
    "\n",
    "pipeline = Pipeline(steps=pipeline_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e02eaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train.drop(columns=[label_column]), X_train[label_column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdb6a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = dict(pipeline.steps)[\"preprocessor\"].transformers[1][1].named_steps[\"OneHotEncoder\"]\n",
    "imputer = dict(pipeline.steps)[\"preprocessor\"].transformers[1][1].named_steps[\"SimpleImputer\"]\n",
    "cat_data = X_train[categorical_cols_to_transform].copy()\n",
    "\n",
    "data = imputer.fit_transform(cat_data)\n",
    "one_hot.fit(data)\n",
    "#one_hot = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "#one_hot.fit(cat_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68ed139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As onehot encoding changes column count and doesnt return column names, we need to extract the column names. We do this also for the feature selector transformers.\n",
    "onehot_encoder_columns = get_onehot_encoder_names(dict(pipeline.steps)[\"preprocessor\"].transformers[1][1].named_steps[\"OneHotEncoder\"], categorical_columns)\n",
    "\n",
    "col_names_ = numeric_columns + onehot_encoder_columns + [col for col in list(X_train.drop(columns=[label_column])) if col not in numeric_columns and col not in categorical_columns]\n",
    "\n",
    "feature_selector_indices = np.where(dict(pipeline.steps)['feature_selector'].get_support()==True)[0]\n",
    "col_names = [col_names_[i] for i in feature_selector_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52f6840",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed = pipeline.transform(X_train.drop(columns=[label_column]))\n",
    "X_train_df = pd.DataFrame(X_train_transformed, columns=col_names)\n",
    "X_train_df[label_column] = X_train[label_column].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e99789",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(f\"X Train has {X_train_transformed.shape[0]} rows and {X_train_transformed.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e615f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sample data after applying the transformations:\")\n",
    "X_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a8c198",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_data_pipeline_pkl = notebook_config[\"output_files\"][\"data_pipeline_file\"]\n",
    "file_output_column_names_pkl = notebook_config[\"output_files\"][\"final_column_names_file\"]\n",
    "\n",
    "logging.info(\"Dumping the pipeline pickle and column names in output directory\")\n",
    "with open(os.path.join(model_artifacts_dir, file_data_pipeline_pkl), 'wb') as f:\n",
    "    pickle.dump(pipeline, f)\n",
    "\n",
    "with open(os.path.join(model_artifacts_dir, file_output_column_names_pkl), \"wb\") as f:\n",
    "    pickle.dump(col_names, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5817e66b",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14630bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7aa4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Getting feature importance by fitting an xgb model\")\n",
    "\n",
    "xgboost_model = XGBClassifier()\n",
    "xgboost_model.fit(X_train_df.drop(columns=[label_column]), X_train[label_column].astype(int));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1e1f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature importances:\\n\")\n",
    "feature_importances_xgboost = xgboost_model.feature_importances_\n",
    "print(f\"{'Feature':<50}: {'Score'}\")\n",
    "for feature_id in np.argsort(feature_importances_xgboost)[::-1]:\n",
    "    if feature_importances_xgboost[feature_id]==0:\n",
    "        print(\"\\n\\nRest all features have importance score 0.\")\n",
    "        break\n",
    "    print(f\"{X_train_df.columns[feature_id]:<50}: {feature_importances_xgboost[feature_id]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c35848e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(local_output_path, \"feature_importances_xgb.json\"), \"w\") as f:\n",
    "    json.dump(dict(zip(list(X_train_df), [float(f) for f in feature_importances_xgboost])), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5eecd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bar_plot(feature_importances: list, X_train: pd.DataFrame, top_features: int=30):\n",
    "    \"\"\"\n",
    "    Create a bar plot of features against their corresponding feature importance score.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    feature_importances_sorted_idx = np.argsort(feature_importances)[::-1][:top_features]\n",
    "    feature_importances_sorted = feature_importances[feature_importances_sorted_idx]\n",
    "    column_names = [list(X_train)[idx] for idx in feature_importances_sorted_idx]\n",
    "    x_indices = [_ for _ in range(len(column_names))]\n",
    "    plt.bar(x_indices, feature_importances_sorted, color=\"blue\")\n",
    "    plt.xticks(x_indices, column_names, rotation=90)\n",
    "    plt.xlabel(\"Feature\", fontsize=18)\n",
    "    plt.ylabel(\"Importance Score\", fontsize=18)\n",
    "    plt.title(f\"XGBoost based Feature Importance Scores of top {top_features} features\", fontsize=18)\n",
    "    plt.savefig(os.path.join(local_output_path, f\"feature_importances_xgb.{IMAGE_FORMAT}\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d102ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_bar_plot(feature_importances_xgboost, X_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00eb65da",
   "metadata": {},
   "source": [
    "### Saving train test val datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada16617",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_transformed = pipeline.transform(X_val.drop(columns=[label_column]))\n",
    "X_val_df = pd.DataFrame(X_val_transformed, columns=col_names)\n",
    "X_val_df[label_column] = X_val[label_column].values\n",
    "\n",
    "X_test_transformed = pipeline.transform(X_test.drop(columns=[label_column]))\n",
    "X_test_df = pd.DataFrame(X_test_transformed, columns=col_names)\n",
    "X_test_df[label_column] = X_test[label_column].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009e26ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Dumping train, val, test datasets\")\n",
    "X_train_df.to_csv(os.path.join(local_output_path, \"train.csv\"), index=False)\n",
    "X_val_df.to_csv(os.path.join(local_output_path, \"val.csv\"), index=False)\n",
    "X_test_df.to_csv(os.path.join(local_output_path, \"test.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b0e2aa",
   "metadata": {},
   "source": [
    "### Uploading pre-processing pipeline files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b834f6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Uploading preprocessing files to staging location\")\n",
    "for filename in [file_data_pipeline_pkl, file_output_column_names_pkl]\n",
    "    print(f\"Uploading {filename} to s3\")\n",
    "    upload_file_to_s3(\n",
    "        creds = creds_config,\n",
    "        local_file_path = f\"{model_artifacts_dir}/{filename}\",\n",
    "        s3_bucket_name = creds_config[\"aws\"][\"s3Bucket\"],\n",
    "        s3_path = f\"{creds_config['aws']['staging_models_s3_prefix']}/{job_id}/{filename}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d11067",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell to hide code while converting to a html page\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "$('div.input').hide();\n",
    "</script>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcc4266",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
