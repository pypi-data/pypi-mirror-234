{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04d2f8de",
   "metadata": {
    "tags": []
   },
   "source": [
    "## README:\n",
    "\n",
    "App : **Sample Application**\n",
    "\n",
    "Stage : **Training**\n",
    "\n",
    "This is the sample notebook for loading data from s3 location \n",
    "\n",
    "The notebook expects the required inputs in the adjacent `data` folder \n",
    "\n",
    "Loading configuration from `config/train.yaml` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee3e37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import joblib\n",
    "import json\n",
    "import logging\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import display\n",
    "from pprint import pprint\n",
    "from typing import Dict, List, Tuple, Union\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "#Rudderlab data utilities imports\n",
    "from rudderlabs.data.apps.log import setup_file_logger\n",
    "from rudderlabs.data.apps.config import read_yaml\n",
    "from rudderlabs.data.apps.utils import get_latest_folder\n",
    "from rudderlabs.data.apps.aws.s3 import upload_file_to_s3\n",
    "\n",
    "from sklearn.metrics import average_precision_score, precision_recall_fscore_support, roc_auc_score, f1_score\n",
    "from sklearn.metrics import precision_score, recall_score, precision_recall_curve, roc_curve\n",
    "from sklearn.metrics import get_scorer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "\n",
    "pd.options.display.max_columns=None\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0e3000",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters cell for papermill. These values can get overridden by parameters passed by papermill\n",
    "job_id = None\n",
    "train_id = None\n",
    "local_input_path = None\n",
    "local_output_path = None\n",
    "code_path = \"../\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70d5e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not job_id:\n",
    "    job_id = get_latest_folder(\"../data\").split(\"/\")[-1]\n",
    "    print(f\"Data prep run id is not given. Taking the latest run id: {job_id}\")\n",
    "\n",
    "job_id = str(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2b1743",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize input and output paths if they are not passed by papermill\n",
    "if local_input_path is None:\n",
    "    local_input_path = f\"../data/{job_id}/data-prep\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff97fa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_id(job_id_folder):\n",
    "    train_id = 0\n",
    "    while True:\n",
    "        if not os.path.exists(os.path.join(job_id_folder, f\"train_{train_id}\")):\n",
    "            # The train id doesn't exist. This will be used as incremental\n",
    "            return train_id\n",
    "        else:\n",
    "            train_id+=1\n",
    "\n",
    "if train_id is None:\n",
    "    train_id = generate_train_id(local_input_path)\n",
    "print(f\"Train job with id {train_id} is starting\")\n",
    "\n",
    "\n",
    "if local_output_path is None:\n",
    "    local_output_path = f\"../data/{job_id}/train_{train_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2be85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(job_id)\n",
    "print(f\"local_input_path {local_input_path}\")\n",
    "print(f\"local_output_path {local_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e33bdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Local imports\n",
    "sys.path.append(code_path)\n",
    "from data_loader import DataIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e35075f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "# All the required constants are defined here\n",
    "IMAGE_FORMAT = 'png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7983552",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logging setup\n",
    "try:\n",
    "    log_file_path = os.path.join(local_output_path, \"logs\", \"train.log\")\n",
    "    logging = setup_file_logger(log_file_path)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "logging.info(\"\\n\\n\\t\\tSTARTING TRAINING JOB\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6405ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configurations\n",
    "notebook_config = read_yaml(os.path.join(code_path, \"config/train.yaml\"))\n",
    "print(\"Notebook config:\")\n",
    "pprint(notebook_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb663a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "creds_config = read_yaml(os.path.join(code_path, \"credentials.yaml\"))\n",
    "print(\"Credentials config:\")\n",
    "pprint(creds_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825d861f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the output files get stored in the output_directory. Each run of the feature_processing generates a new sub directory based on the timestamp.\n",
    "# output directory structure\n",
    "# - data\n",
    "#   - <job_id>\n",
    "#       - train\n",
    "#           - visuals\n",
    "#           - model_artifacts\n",
    "visuals_dir = os.path.join( local_output_path, \"visuals\" )\n",
    "model_artifacts_dir = os.path.join(local_output_path, \"model_artifacts\")\n",
    "\n",
    "logging.info(f\"All the output files will be saved to following location: {local_output_path}\")\n",
    "for output_path in [local_output_path, visuals_dir, model_artifacts_dir]:\n",
    "    Path(output_path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5964fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = \"train\"\n",
    "TEST = \"test\"\n",
    "VAL = \"val\"\n",
    "\n",
    "dataframes = { \n",
    "    split : pd.read_csv(os.path.join(local_input_path, f\"{split}.csv\"))\n",
    "    for split in [TRAIN, TEST, VAL]\n",
    "}\n",
    "\n",
    "print(\"No:of samples in :\")\n",
    "for split, df in dataframes.items():\n",
    "    print(f\"{split}: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b37211",
   "metadata": {},
   "source": [
    "Sample Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c60043a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes[TRAIN].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce84fff-7cb2-4cf6-b4f5-bccf0fa818b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,3,figsize=(16,6))\n",
    "fig.suptitle(\"Label distribution:\")\n",
    "for n, (split, df) in enumerate(dataframes.items()):\n",
    "    df[notebook_config[\"data_path\"][\"label_col_name\"]].value_counts().plot.pie(explode=[0,0.1], autopct=\"%1.1f%%\", ax=axs[n])\n",
    "    axs[n].set_title(split)\n",
    "    axs[n].set_ylabel(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6324a5d7-28ef-4316-b85c-b58949511469",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = notebook_config[\"model_params\"][\"models\"]\n",
    "pprint(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a846d4e-ed53-4503-b83b-82ada5ca52b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_run = [model[\"name\"] for model in models]\n",
    "sep = ', '\n",
    "print(f\"Models to explore are: {sep.join(models_to_run)}\")\n",
    "print(\"**Note: Currently only xgboost is implemented for the POC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8cbbc6-4e70-4053-972f-78d00a2cec08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate input data and label data for the given dataset\n",
    "def prepare_dataset(dataset: pd.DataFrame, label_columns: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    return dataset.drop(columns=label_columns), dataset[label_columns]\n",
    "\n",
    "label_columns = notebook_config[\"data_path\"][\"label_col_name\"]\n",
    "X_train, y_train = prepare_dataset(dataframes[TRAIN], label_columns)\n",
    "X_test, y_test = prepare_dataset(dataframes[TEST], label_columns)\n",
    "X_val, y_val = prepare_dataset(dataframes[VAL], label_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3affb32a-17a5-43a7-90d5-5dd613783066",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperopts_expressions_map = {exp.__name__: exp for exp in [hp.choice, hp.quniform, hp.uniform, hp.loguniform]}\n",
    "evalution_metrics_map = {metric.__name__: metric for metric in [average_precision_score, precision_recall_fscore_support]}\n",
    "\n",
    "#Generate hyper parameter space for given options\n",
    "def generate_hyperparameter_space(hyperopts: List[dict]):\n",
    "    space = {}\n",
    "    for expression in hyperopts:\n",
    "        expression = expression.copy()\n",
    "        exp_type = expression.pop(\"type\")\n",
    "        name = expression.pop(\"name\")\n",
    "\n",
    "        # Handle expression for explicit choices and \n",
    "        # implicit choices using \"low\", \"high\" and optinal \"step\" values\n",
    "        if exp_type == \"choice\":\n",
    "            options = expression[\"options\"]\n",
    "            if not isinstance(options, list):\n",
    "                expression[\"options\"] = list(range( options[\"low\"], options[\"high\"], options.get(\"step\", 1)))\n",
    "                \n",
    "        space[name] = hyperopts_expressions_map[f\"hp_{exp_type}\"](name, **expression)\n",
    "    return space\n",
    "\n",
    "\n",
    "def build_model(\n",
    "    X_train:pd.DataFrame, y_train:pd.DataFrame,\n",
    "    X_val:pd.DataFrame, y_val:pd.DataFrame,\n",
    "    model_class: Union[XGBClassifier, RandomForestClassifier, MLPClassifier], \n",
    "    model_config: dict, \n",
    "    ):\n",
    "    \n",
    "    hyperopt_space = generate_hyperparameter_space(model_config[\"hyperopts\"])\n",
    "\n",
    "    #We can set evaluation set for xgboost model which we cannot directly configure from configuration file\n",
    "    fit_params = model_config.get(\"fitparams\", {}).copy()\n",
    "    if model_class.__name__ == \"XGBClassifier\":\n",
    "        fit_params[\"eval_set\"] = [( X_train, y_train), ( X_val, y_val)]\n",
    "\n",
    "    #Objective method to run for different hyper-parameter space\n",
    "    def objective(space):\n",
    "        clf = model_class(**model_config[\"modelparams\"], **space)\n",
    "        clf.fit(X_train, y_train, **fit_params)\n",
    "        pred = clf.predict_proba(X_val)\n",
    "        eval_metric_name = model_config[\"evaluation_metric\"]\n",
    "        pr_auc = evalution_metrics_map[eval_metric_name](y_val, pred[:, 1])\n",
    "        \n",
    "        return {'loss': (0  - pr_auc), 'status': STATUS_OK , \"config\": space}\n",
    "\n",
    "    trials = Trials()\n",
    "    best_hyperparams = fmin(fn = objective,\n",
    "                            space = hyperopt_space,\n",
    "                            algo = tpe.suggest,\n",
    "                            max_evals = model_config[\"hyperopts_config\"][\"max_evals\"],\n",
    "                            return_argmin=False,\n",
    "                            trials = trials)\n",
    "\n",
    "    clf = model_class(**best_hyperparams, **model_config[\"modelparams\"])\n",
    "    clf.fit(X_train, y_train, **fit_params)\n",
    "    return clf, trials, best_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea197a55-7d72-4ec2-87dc-2ef400b2f956",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models_map = { model.__name__: model for model in [XGBClassifier, RandomForestClassifier, MLPClassifier]}\n",
    "models_hyper_parameters_search_results = {}\n",
    "\n",
    "for model_config in models:\n",
    "    name = model_config[\"name\"]\n",
    "    print(f\"Training {name}\")\n",
    "    clf, trials, best_hyperparams = build_model(X_train, y_train, X_val, y_val, models_map[name], model_config)\n",
    "\n",
    "    models_hyper_parameters_search_results[name] = {\n",
    "        \"model\" : clf,\n",
    "        \"trials\" : trials,\n",
    "        \"eval_metric_name\" : model_config[\"evaluation_metric\"],\n",
    "        \"eval_metric_values\" : [ -1*loss for loss in trials.losses()],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cb3447-aa4c-476f-8390-63ad7c42a90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hyperparameter_search_resulsts(name:str, values:list, model_name:str) -> None:\n",
    "    plt.figure(figsize=(15,6))\n",
    "    plt.plot(values)\n",
    "    plt.xlabel(\"Trial\")\n",
    "    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    plt.ylabel(name)\n",
    "    plt.grid(True)\n",
    "    plt.title(f\"Validation score on various hyperparam combination steps ({model_name})\");\n",
    "\n",
    "for model_name, results in models_hyper_parameters_search_results.items():\n",
    "    plot_hyperparameter_search_resulsts(results[\"eval_metric_name\"], results[\"eval_metric_values\"], model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c230b04-2371-44fa-870a-8dc8668e90c5",
   "metadata": {},
   "source": [
    "**Top runs with their hyperparameters:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26c43d6-0d12-4d2f-a521-be2d2af1c16e",
   "metadata": {},
   "source": [
    "## Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8368a1b-9780-47b9-bad1-2aa476f897c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_metrics(y_true, y_pred_proba, th=0.5):\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, np.where(y_pred_proba>th,1,0))\n",
    "    precision = precision[1]\n",
    "    recall = recall[1]\n",
    "    f1 = f1[1]\n",
    "    roc_auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    pr_auc = average_precision_score(y_true, y_pred_proba)\n",
    "    metrics = {\"precision\": precision, \"recall\": recall, \"f1_score\": f1, \"roc_auc\": roc_auc, 'pr_auc': pr_auc}\n",
    "    return metrics\n",
    "    \n",
    "def get_best_th(y_true, y_pred_proba):\n",
    "    \"\"\"\n",
    "    Returns the threshold that maximizes f1 score based on y_true and y_pred_proba\n",
    "    Also returns the metrics at the threshold\n",
    "    y_true: Array of 1s and 0s. True labels\n",
    "    y_pred_proba: Array of predicted probabilities\n",
    "    \"\"\"\n",
    "    best_f1 = 0.0\n",
    "    best_th = 0.0\n",
    "\n",
    "    for th in np.arange(0,1,0.01):\n",
    "        f1 = f1_score(y_true, np.where(y_pred_proba>th,1,0))\n",
    "        if f1 >= best_f1:\n",
    "            best_th = th\n",
    "            best_f1 = f1\n",
    "            \n",
    "    best_metrics = get_classification_metrics(y_true, y_pred_proba, best_th)\n",
    "    return best_metrics, best_th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48b5baf-168f-4b76-8fbe-78c6766f927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(\n",
    "    clf: Union[XGBClassifier, RandomForestClassifier, MLPClassifier],\n",
    "    X_train: pd.DataFrame, y_train: pd.DataFrame,\n",
    "    X_test: pd.DataFrame, y_test: pd.DataFrame,\n",
    "    X_val: pd.DataFrame, y_val: pd.DataFrame\n",
    "):\n",
    "    train_preds = clf.predict_proba(X_train)[:,1]\n",
    "    train_metrics, prob_threshold = get_best_th(y_train, train_preds)\n",
    "\n",
    "    test_preds = clf.predict_proba(X_test)[:,1]\n",
    "    test_metrics = get_classification_metrics(y_test, test_preds, prob_threshold)\n",
    "\n",
    "    val_preds = clf.predict_proba(X_val)[:,1]\n",
    "    val_metrics = get_classification_metrics(y_val, val_preds, prob_threshold)\n",
    "\n",
    "    metrics = {\"train\": train_metrics, \"val\": val_metrics, \"test\": test_metrics}\n",
    "    predictions = {\"train\": train_preds, \"val\": val_preds, \"test\": test_preds}\n",
    "     \n",
    "    return metrics, predictions, prob_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e47d88a-6cb9-4af6-ad65-98bd8b746d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_evaluations_results = {}\n",
    "\n",
    "for model_name, results in models_hyper_parameters_search_results.items():\n",
    "    metrics, predictions, prob_threshold = get_metrics(results[\"model\"], X_train, y_train, X_test, y_test, X_val, y_val)\n",
    "    models_evaluations_results[model_name] = {\n",
    "        \"metrics\" : metrics,\n",
    "        \"predictions\" : predictions,\n",
    "        \"prob_threshold\" : prob_threshold\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{model_name}\")\n",
    "    results_df = pd.DataFrame.from_records(metrics).T.round(3)\n",
    "    display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dced40c",
   "metadata": {},
   "source": [
    "### Best Model\n",
    "\n",
    "Selecting best model based on F1 score for validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5260cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: evaluation_metric should be used here too, to pick the best model. Currently it is hard coded as f1_score\n",
    "f1_scores = { name:results[\"metrics\"][\"val\"][\"f1_score\"] for name, results in models_evaluations_results.items()}\n",
    "f1_scores = dict(sorted(f1_scores.items(), key=lambda item: item[1], reverse=True))\n",
    "best_model_name = list(f1_scores.keys())[0]\n",
    "best_model = models_hyper_parameters_search_results[best_model_name][\"model\"]\n",
    "print(f\"Selected best model {best_model_name} with f1 score {f1_scores[best_model_name]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7202098-bae1-4c12-b391-3645d0bfc133",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, precision_recall_curve,confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dd8bab-312a-49f4-8410-e46035c4fe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actuals = {\"train\": y_train, \"test\": y_test, \"val\": y_val}\n",
    "y_preds = models_evaluations_results[best_model_name][\"predictions\"]\n",
    "prob_threshold = models_evaluations_results[best_model_name][\"prob_threshold\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79ad73e-82f7-48ea-b0d5-a416ee63f53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(constrained_layout=True, figsize=(16,15))\n",
    "fig.suptitle(\"Confusion Matrices:\")\n",
    "ax_dict = fig.subplot_mosaic(\n",
    "    [\n",
    "        [\"train-cm\", \"val-cm\", \"test-cm\"],\n",
    "        [\"pr-auc\", \"pr-auc\", \"pr-auc\"],\n",
    "        [\"roc-auc\", \"roc-auc\", \"roc-auc\"],\n",
    "    ],\n",
    ")\n",
    "pr_aucs  = []\n",
    "roc_aucs = []\n",
    "for n,split in enumerate([\"train\", \"val\", \"test\"]):\n",
    "    y_actual = y_actuals[split]\n",
    "    y_pred = y_preds[split]\n",
    "    dsp = ConfusionMatrixDisplay(confusion_matrix(y_actual, np.where(y_pred>=prob_threshold,1,0)))\n",
    "    dsp.plot(ax=ax_dict[f\"{split}-cm\"])\n",
    "    ax_dict[f\"{split}-cm\"].set_title(f\"{split}\")\n",
    "    pr, re, th =  precision_recall_curve(y_actual, y_pred)\n",
    "    sns.lineplot(x=pr, y=re, ax=ax_dict[\"pr-auc\"])\n",
    "    \n",
    "    pr_aucs.append(average_precision_score(y_actual, y_pred))\n",
    "    fpr, tpr, _ = roc_curve(y_actual, y_pred)\n",
    "    roc_aucs.append(auc(fpr, tpr))\n",
    "    sns.lineplot(x=fpr, y=tpr, ax=ax_dict[\"roc-auc\"])\n",
    "    \n",
    "ax_dict[\"pr-auc\"].set_title(\"Precision Recall AUC\")\n",
    "ax_dict[\"pr-auc\"].legend([f\"train ({pr_aucs[0]:.2f})\", f\"val ({pr_aucs[1]:.2f})\", f\"test ({pr_aucs[2]:.2f})\"])\n",
    "\n",
    "ax_dict[\"pr-auc\"].set_xlabel(\"Precision\")\n",
    "ax_dict[\"pr-auc\"].set_ylabel(\"Recall\")\n",
    "ax_dict[\"pr-auc\"].grid(True)\n",
    "\n",
    "\n",
    "ax_dict[\"roc-auc\"].set_title(\"ROC AUC\")\n",
    "ax_dict[\"roc-auc\"].legend([f\"train ({roc_aucs[0]:.2f})\", f\"val ({roc_aucs[1]:.2f})\", f\"test ({roc_aucs[2]:.2f})\"])\n",
    "\n",
    "ax_dict[\"roc-auc\"].set_xlabel(\"FPR\")\n",
    "ax_dict[\"roc-auc\"].set_ylabel(\"TPR\")\n",
    "ax_dict[\"roc-auc\"].grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165d33ee-6fd6-4121-8046-ba8f1f97d08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#save model\n",
    "joblib.dump(best_model, f\"{model_artifacts_dir}/saved_model.pkl\")\n",
    "print(f\"Model is saved as a pickle file in the location:\\n\\t{local_output_path}\\n\\nIt can be loaded using joblib.load\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a379df",
   "metadata": {},
   "source": [
    "### Uploading Model files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b02d7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Uploading saved model file\")\n",
    "upload_file_to_s3(\n",
    "    creds = creds_config,\n",
    "    local_file_path = f\"{model_artifacts_dir}/saved_model.pkl\",\n",
    "    s3_bucket_name = creds_config[\"aws\"][\"s3Bucket\"],\n",
    "    s3_path = f\"{creds_config['aws']['staging_models_s3_prefix']}/{job_id}/saved_model.pkl\"\n",
    ")\n",
    "\n",
    "s3_location = f\"s3://{creds_config['aws']['s3Bucket']}/{creds_config['aws']['staging_models_s3_prefix']}/{job_id}\"\n",
    "print(f\" Model file is uploaded to:\\n\\t{s3_location}\")\n",
    "logging.info(f\" Model file is uploaded to:\\n\\t{s3_location}\")\n",
    "\n",
    "metrics_dict = {}\n",
    "\n",
    "#Run metrics on golden dataset\n",
    "for metric in notebook_config[\"model_params\"][\"evaluation_metrics\"]:\n",
    "    scorer = get_scorer(metric)\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        metrics_dict[f\"{split}_{metric}\"] = scorer._score_func(y_actuals[split], np.where(y_preds[split] >  prob_threshold,1,0))\n",
    "\n",
    "print(\"Adding entry to model registry\")\n",
    "data = {\n",
    "    \"job_id\": job_id,\n",
    "    \"model_name\" : \"leadscoring\",\n",
    "    \"model_type\" : \"staging\",\n",
    "    \"timestamp\" : datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"threshold\" : prob_threshold,\n",
    "    \"metrics\" : json.dumps(metrics_dict),\n",
    "    \"model_files_location\" : s3_location,\n",
    "    \"version\" : \"1.0.0\"\n",
    "}\n",
    "print(f\"Adding entry to model registry:\\n{data}\")\n",
    "logging.info(f\"Adding entry to model registry:\\n{data}\")\n",
    "\n",
    "data = pd.DataFrame(data, index=[0])\n",
    "data_io = DataIO(notebook_config, creds_config)\n",
    "\n",
    "data_io.write_to_wh_table(\n",
    "    df = data,\n",
    "    table_name = creds_config[\"data_warehouse\"][\"model_registry_table\"],\n",
    "    schema = creds_config[\"data_warehouse\"][\"schema\"],\n",
    "    if_exists = \"append\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d11067",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cell to hide code while converting to a html page\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "$('div.input').hide();\n",
    "</script>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d1f420",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('rlabs')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "fb9dda7c9d815e1cdc337d7fe8d5832923daae4c84b0a4e15a32dd1f30943ba5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
