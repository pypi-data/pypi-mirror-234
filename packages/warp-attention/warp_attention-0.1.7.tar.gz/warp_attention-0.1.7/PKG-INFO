Metadata-Version: 2.1
Name: warp_attention
Version: 0.1.7
Summary: Warp attention: hardware efficient implementation of scaled dot product attention.
Author: demoriarty
Keywords: transformers,attention,scaled dot product attention,pytorch
