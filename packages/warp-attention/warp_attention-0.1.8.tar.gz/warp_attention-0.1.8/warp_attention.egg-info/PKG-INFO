Metadata-Version: 2.1
Name: warp-attention
Version: 0.1.8
Summary: Warp attention: hardware efficient implementation of scaled dot product attention.
Author: demoriarty
Keywords: transformers,attention,scaled dot product attention,pytorch
