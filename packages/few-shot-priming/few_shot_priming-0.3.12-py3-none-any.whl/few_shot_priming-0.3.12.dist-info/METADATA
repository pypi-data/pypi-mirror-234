Metadata-Version: 2.1
Name: few-shot-priming
Version: 0.3.12
Summary: Analyzing priming effects in a few shot setting environment
Home-page: https://gitlab.uni-hannover.de/y.ajjour/few-shot-priming
Author: Yamen Ajjour
Author-email: yajjour@hotmail.com
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Description-Content-Type: text/markdown
Requires-Dist: transformers (>=4.26.1)
Requires-Dist: pandas
Requires-Dist: openprompt
Requires-Dist: scikit-learn
Requires-Dist: torch
Requires-Dist: pyyaml
Requires-Dist: wandb
Requires-Dist: debater-python-api
Requires-Dist: gensim
Requires-Dist: accelerate
Requires-Dist: peft
Requires-Dist: dask[dataframe]

# Analyzing Priming Effect in Prompt-based learning

How does priming affect prompt-based learing?
This project aims at analyzing this effect in stance classification.
We train a stance classifier on the ibm stance classification dataset by
fine-tuning a GPT-2 model with a prompt and analzing how does the selection
of the few shots used in the prompt affect the performance of the model.
Our main assumption is that the examples chosen should be chosen in a diverse manner with regard topic.
1) To evaluate the prompt-fine-tuning, run the following command
* Hyperparamter optimization
```
python scripts/run_prompt_fine_tuning.py --validate --optimize 
```

* Best Hyperparameters
```
python scripts/run_prompt_fine_tuning.py --validate --optimize 
``` 
2) To evaluate the in-context (prompt) setup run
```
python scripts/run_prompting.py --validate --optimize 
```
3) To evaluate DeBERTa (a normal classifier) with all hyperparameters, run the following
```
python scripts/optimize_baseline.py 
```

4) To evaluate Alpaca in a instructional tuning model run the following:
```
/run_prompt_fine_tuning.py --validate --optimize --alpaca
```

The results of the experiments will be logged to your home directory.
The parameters can be saved in [config.yaml](../blob/maser/config.yaml)
## Priming Sampling strategies
To run an experiment with a topic sampling strategy use the parameter --topic-similar or --t
## Topic Similarity
Examples on similar or diverse topics are sampled using a topic similarity, which relies on a neural
topic modeling (Contextual Topic Model). The Contextual Topic Models is fine-tuned on the validation set
and the cosine similarities between all test and training instances are calculated and saved. While training
the similarities can be used to apply the right sampling strategy.

1) To create a topic model on the validation set, run
```
python scripts/run_topic_modeling.py --create-model --validate
```
For training on the test set, drop --validate
2) To create a baseline (lda and sentence-transformers) on the validation set, run
```
python scripts/run_topic_modeling.py --create-baseline --validate
```
For training on the test set, drop --validate
3) To evaluate the topic models and baseline, run 
```
python scripts/run_topic_modeling.py --evaluate-model --validate
```
4) To compute the similarities between all the validation and training arguments run the following
```
python scripts/run_topic_modeling.py --compute-similarity --validate
```
To load similarities from the code you can use the

```similarities = load_similarities("ibmsc", "validation")```

which returns a numpy matrix of dimension two that has on the first dimension the validation instances and on the second dimension
the indices of the training set.

To find similar or diverse arguments for an argument in the validation or test set, you can use

```examples = sample_diverse(test_istance_index, similarities, df_training, few_shot_size)```

similar can be used for sample_similar

```examples = sample_similar(test_istance_index, similarities, df_training, few_shot_size)```
Notice that you have to load the test and training instances in the same order as the one used for training the topic model.
