Metadata-Version: 2.1
Name: aiforthechurch
Version: 0.5
Summary: Package for training and deploying doctrinally correct LLMs.
Home-page: https://github.com/trialan/aiforthechurch
Author: Andrew Rogers, Thomas Rialan
Author-email: andrew@biblechat.ai
Description-Content-Type: text/markdown
Requires-Dist: langdetect
Requires-Dist: torch >=2.0.1
Requires-Dist: peft >=0.5.0
Requires-Dist: transformers >=4.34.0
Requires-Dist: accelerate ==0.23.0
Requires-Dist: bitsandbytes >=0.41.1

# AI for the Church
Modern LLMs are rooted in secular value systems that are often misaligned with religious organisations. This PyPI package allows anyone to train and deploying doctrinally correct LLMs based on Llama2. Effectively, we are aligning models to a set of values.

## Model fine-tuning
```python
from aiforthechurch import align_llama2
doctrinal_dataset = "/path/to/csv"
align_llama2(doctrinal_dataset)
```

`aiforthechurch` is integrated with HuggingFace shuch that the aligned model will be automatically pushed to your HuggingFace repo of choice at the end of the training.

At aiforthechurch.org we provide tools for generating doctrinal datasets, a few examples are available at huggingface.co/AiForTheChurch.

## Model inference and deployment
We implemented an inference API in the same format as OpenAI's.

```python
import aiforthechurch
aiforthechurch.Completion.create(denomination="catholic", message="Does Jesus love me?")
```

There is also an asynchronous streaming API, just set `stream=True`.

And you can use our code to create your own inference server with the models
you train by editing the `MODELS` dictionary in
`gloohack/deployment/prod_models.py` with a denomination and a path to your
model in huggingface. Start the server by running the following command on your
machine:

```
python gloohack/deployment/inference.py
```

## Model training requirements
If you wish to train your models using this repo you will need access to a machine with over 16GB of GPU memory and 30GB RAM. The full model weights for Llama2-7B amount to almost 30GB, but we use parameter-efficient fine-tuning (PEFT) LoRA to save memory and avoid any catastrophic forgetting during the fine-tuning procedure.


## References
We leaned heavily on open-source libraries like `transformers`, `peft`, and
`bitsandbytes` for this project.
- Dettmers, Tim, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. 2022. "[LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale](https://arxiv.org/abs/2208.07339)." *arXiv preprint arXiv:2208.07339*.
- Hu, Edward J., Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. "[LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)." *arXiv preprint arXiv:2106.09685*.
