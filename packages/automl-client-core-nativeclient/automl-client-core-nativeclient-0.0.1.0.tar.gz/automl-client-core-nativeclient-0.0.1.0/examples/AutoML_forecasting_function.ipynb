{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "print(azureml.core.__version__)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import warnings\n",
    "# Squash warning messages for cleaner output in the notebook\n",
    "warnings.showwarning = lambda *args, **kwargs: None\n",
    "\n",
    "from pandas.tseries.frequencies import to_offset\n",
    "\n",
    "from automl.client.core.nativeclient import AutoMLForecaster\n",
    "from automl.client.core.nativeclient import AutoMLNativeClient\n",
    "from azureml.core.authentication import AbstractAuthentication\n",
    "\n",
    "np.set_printoptions(precision=8, suppress=True, linewidth=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authenticate to the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "# Create the workspace using the specified parameters\n",
    "workspace = Workspace.create(name = 'workspace name',\n",
    "                             subscription_id = 'subscription id',\n",
    "                             resource_group = 'resource group name', \n",
    "                             location = 'region',\n",
    "                             create_resource_group = True,\n",
    "                             sku = 'basic',\n",
    "                             exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "For the demonstration purposes we will generate the data artificially and use them for the forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_COLUMN_NAME = 'date'\n",
    "GRAIN_COLUMN_NAME = 'grain'\n",
    "TARGET_COLUMN_NAME = 'y'\n",
    "\n",
    "def get_timeseries(train_len: int,\n",
    "                   test_len: int,\n",
    "                   time_column_name: str,\n",
    "                   target_column_name: str,\n",
    "                   grain_column_name: str,\n",
    "                   grains: int = 1,\n",
    "                   freq: str = 'H'):\n",
    "    \"\"\"\n",
    "    Return the time series of designed length.\n",
    "\n",
    "    :param train_len: The length of training data (one series).\n",
    "    :type train_len: int\n",
    "    :param test_len: The length of testing data (one series).\n",
    "    :type test_len: int\n",
    "    :param time_column_name: The desired name of a time column.\n",
    "    :type time_column_name: str\n",
    "    :param\n",
    "    :param grains: The number of grains.\n",
    "    :type grains: int\n",
    "    :param freq: The frequency string representing pandas offset.\n",
    "                 see https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html\n",
    "    :type freq: str\n",
    "    :returns: the tuple of train and test data sets.\n",
    "    :rtype: tuple\n",
    "\n",
    "    \"\"\"\n",
    "    data_train = []  # type: List[pd.DataFrame]\n",
    "    data_test = []  # type: List[pd.DataFrame]\n",
    "    data_length = train_len + test_len\n",
    "    for i in range(grains):\n",
    "        X = pd.DataFrame({\n",
    "            time_column_name: pd.date_range(start='2000-01-01',\n",
    "                                            periods=data_length,\n",
    "                                            freq=freq),\n",
    "            target_column_name: np.arange(data_length).astype(float),\n",
    "            'universal_answer': np.repeat(42, data_length),\n",
    "            grain_column_name: np.repeat('g{}'.format(i), data_length)\n",
    "        })\n",
    "        data_train.append(X[:train_len])\n",
    "        data_test.append(X[train_len:])\n",
    "    X_train = pd.concat(data_train)\n",
    "    y_train = X_train.pop(target_column_name).values\n",
    "    X_test = pd.concat(data_test)\n",
    "    y_test = X_test.pop(target_column_name).values\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "n_test_periods = 6\n",
    "n_train_periods = 12\n",
    "X_train, y_train, X_test, y_test = get_timeseries(train_len=n_train_periods,\n",
    "                                                  test_len=n_test_periods,\n",
    "                                                  time_column_name=TIME_COLUMN_NAME,\n",
    "                                                  target_column_name=TARGET_COLUMN_NAME,\n",
    "                                                  grain_column_name=GRAIN_COLUMN_NAME,\n",
    "                                                  grains=2)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the configuration and run the forecasting.\n",
    "First generate the configuration, in which we:\n",
    "* Set metadata columns: target, time column and grain column names.\n",
    "* Ask for 10 iterations through models, last of which will represent the Ensemble of previous ones.\n",
    "* Validate our data using cross validation with rolling window method.\n",
    "* Set normalized root mean squared error as a metric to select the best model.\n",
    "* Finally, we set the task to be forecasting.\n",
    "* By default, we apply the lag lead operator and rolling window to the target value i.e. we use the previous values as a predictor for the future ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_settings = {\n",
    "    'primary_metric': 'normalized_root_mean_squared_error',\n",
    "    'iteration_timeout_minutes': 5,\n",
    "    'iterations': 10,\n",
    "    'n_cross_validations': 5,\n",
    "    'time_column_name': TIME_COLUMN_NAME,\n",
    "    'grain_column_names': GRAIN_COLUMN_NAME,\n",
    "    'max_horizon': n_test_periods,\n",
    "    'client': AutoMLNativeClient(workspace=workspace)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the model selection and training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_forecaster = AutoMLForecaster(**time_series_settings)\n",
    "local_run = automl_forecaster.fit(\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    compute_target='local',\n",
    "    show_output=True\n",
    ")\n",
    "# Retrieve the best model to use it further.\n",
    "_, fitted_model = local_run.get_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the fitted model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitted model has two remarkable methods.\n",
    "* predict - for the back compatibility with scikit-learn pipeline. It will return all y values for all horizons and hence its output shape will not align row by row with X_test.\n",
    "* forecast - the method designed specifically for forecasting. It returns the tuple: y values which has the same row number as X_test and the whole transformed data frame. This data frame and y values array will contain data for the most recent origin times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = fitted_model.predict(X_test)\n",
    "print(\"Predicted(forecasted) values size: {}\".format(y_pred.shape[0]))\n",
    "print(\"The number of rows in X_test: {}\".format(X_test.shape[0]))\n",
    "# Note that y_pred cannot be aligned with X_test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional part** <br/>\n",
    "Why y_pred is not aligned to the x test?\n",
    "To answer this question let us dissect fitted_model which is a scikit learn pipeline and make the time series transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default, the first transform in the pipeline is a time series transform.\n",
    "# It generates multiple horizons.\n",
    "# Each step in the scikit learn pipeline is a tuple of name and transform.\n",
    "print(fitted_model.steps[0][0]) # Print the name of 1st transform.\n",
    "# And apply it to X_test\n",
    "X_test_transformed = fitted_model.steps[0][1].transform(X_test)\n",
    "print(\"The number of rows in transformed X_test is: {}\".format(X_test_transformed.shape[0]))\n",
    "# y_pred is aligned with transformed data frame.\n",
    "X_test_transformed['Forecast'] = y_pred\n",
    "# For the brevity we will remove all but horizon and forecast.\n",
    "#X_test_transformed.head()\n",
    "X_transformed_short = X_test_transformed[['horizon_origin', 'Forecast']]\n",
    "X_transformed_short.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_transform is a \"forecast request\", requesting 42 numbers. Multiple values, corresponding to different origin times and therefore horizon, are requested for one forecast period. This is necessary for training and evaluation.<br>\n",
    "Note that now data frame contains the origin time column name. This column is created by the lag lead and rolling window transform. Each new row contain lag and rolling window values corresponding to different horizons. <br>\n",
    "Each line in the data frame is the forecast for period in date using data up to and including origin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End of optional part**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The forecast() function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will review four scenarios.\n",
    "* X_train is followed by the X_test.\n",
    "* There is a gap between X_test and X_train.\n",
    "* There is no gap between X_test and X_train, but y_test is partially known.\n",
    "* No X_test and X_train, were provided, we need to infer all data up to destination date.\n",
    "* The \"ragged\" data: there is context in the y_test, but also there is a gap between X_train and X_test.\n",
    "\n",
    "We will test the accuracy of the predictions using MAPE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### X_train is followed by the X_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_no_gap, xy_nogap =  fitted_model.forecast(X_test, np.repeat(np.NaN, X_test.shape[0]))\n",
    "# xy_nogap contains y_pred_no_gap in the _automl_target_col column.\n",
    "# The data set contains hourly data, the training set ends at 01/01/2000 at  11:00\n",
    "# the testing data start at 12:00\n",
    "xy_nogap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the application of the forecast function only the latest horizins are retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will left joun the X_transformed_short with all horizons to the xy_nogap.\n",
    "# We will trim all predictors and rename the  _automl_target_col and horizon_origin\n",
    "# to Forecast_func and horizon_origin_func\n",
    "xy_short = xy_nogap[['horizon_origin', '_automl_target_col']]\n",
    "xy_short.rename(columns={\"horizon_origin\": \"horizon_origin_func\", \"_automl_target_col\": \"Forecast_func\"}, inplace=True)\n",
    "merged = pd.merge(X_transformed_short, xy_short, how='left', left_index=True, right_index=True)\n",
    "merged.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that ```forecast``` removed all non recent horizons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There is a gap between X_test and X_train.\n",
    "First lets remove by two data points from X_test and then apply the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for grain, df in X_test.groupby(GRAIN_COLUMN_NAME):\n",
    "    dfs.append(df[2:])\n",
    "X_gap = pd.concat(dfs)\n",
    "del dfs # Make sure we cleaned memory after concatenation is complete.\n",
    "# If we have a gap in data, we need to set ignore_data_errors to true,\n",
    "# because otherwise the gap in data will be treated as error.\n",
    "# In this case testing set starts at 14:00\n",
    "y_pred_gap, xy_gap =  fitted_model.forecast(X_gap, np.repeat(np.NaN, X_gap.shape[0]), ignore_data_errors=True)\n",
    "xy_gap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that the result was provided beginning from 14:00."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There is no gap between X_test and X_train, but y_test is partially known.\n",
    "In this example we will generate supply by two y_preds to each grains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[TARGET_COLUMN_NAME] = y_test\n",
    "dfs = []\n",
    "for grain, df in X_test.groupby(GRAIN_COLUMN_NAME):\n",
    "    # We need to group our data frame by grain, and add y back to it\n",
    "    # because we need to make sure that y is aligned to the corresponding X.\n",
    "    y = df[TARGET_COLUMN_NAME]\n",
    "    y[2:] = np.NaN\n",
    "    df[TARGET_COLUMN_NAME] = y\n",
    "    dfs.append(df)\n",
    "X_y = pd.concat(dfs)\n",
    "y_part = X_y.pop(TARGET_COLUMN_NAME).values\n",
    "del dfs # Make sure we cleaned memory after concatenation is complete.\n",
    "y_pred_known, xy_known =  fitted_model.forecast(X_y, y_part)\n",
    "xy_known"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the y passed in was returned back, and NaNs filled in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No X_test and X_train, were provided, we need to infer all data up to destination date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the fair comparison of the results we will take the destination date as a last date in the test set.\n",
    "dest = max(X_test[TIME_COLUMN_NAME])\n",
    "y_pred_dest, xy_dest = fitted_model.forecast(forecast_destination=dest)\n",
    "xy_dest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The \"ragged\" data: there is context in the y_test, but also there is a gap between X_train and X_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will shift X_test by two hours\n",
    "dfs = []\n",
    "for grain, df in X_test.groupby(GRAIN_COLUMN_NAME):\n",
    "    # We need to group our data frame by grain, and add y back to it\n",
    "    # because we need to make sure that y is aligned to the corresponding X.\n",
    "    y = [14., 15., np.NaN, np.NaN, np.NaN, np.NaN]\n",
    "    df[TARGET_COLUMN_NAME] = y\n",
    "    # Shift dates\n",
    "    start = min(df[TIME_COLUMN_NAME]) + 2 * to_offset('H')\n",
    "    df[TIME_COLUMN_NAME] = pd.date_range(start = start, freq='H', periods=df.shape[0])\n",
    "    dfs.append(df)\n",
    "X_ragged = pd.concat(dfs)\n",
    "y_ragged = X_ragged.pop(TARGET_COLUMN_NAME).values\n",
    "y_pred_ragged, xy_ragged = fitted_model.forecast(X_ragged, y_ragged)\n",
    "xy_ragged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the forecast starts at 16:00."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the results using MAPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MAPE function  for the benchmarking our results.\n",
    "def MAPE(actual, pred):\n",
    "    actual_ft = actual.astype(float)\n",
    "    not_na = ~(np.isnan(actual_ft))\n",
    "    not_zero = ~np.isclose(actual_ft, 0.0)\n",
    "    actual_safe = actual_ft[not_na & not_zero]\n",
    "    pred_safe = pred[not_na & not_zero]\n",
    "    APE = 100*np.abs((actual_safe - pred_safe)/actual_safe)\n",
    "    return np.mean(APE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to trim xy_nogap, xy_dest and X_test to make comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_df(X):\n",
    "    dfs = []\n",
    "    for grain, df in X.groupby(GRAIN_COLUMN_NAME):\n",
    "        dfs.append(df[2:])\n",
    "    return pd.concat(dfs)\n",
    "# X_test already contains y.\n",
    "X_test_trimmed = trim_df(X_test)\n",
    "xy_nogap_trimmed = trim_df(xy_nogap)\n",
    "xy_dest_trimmed = trim_df(xy_dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we will be able to estimate MAPE we need to make sure that all values are aligned. We will set index to grain and date and sort data frame with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_trimmed.set_index([TIME_COLUMN_NAME, GRAIN_COLUMN_NAME], inplace=True)\n",
    "X_test_trimmed.sort_index(inplace=True)\n",
    "xy_nogap_trimmed.sort_index(inplace=True)\n",
    "xy_dest_trimmed.sort_index(inplace=True)\n",
    "xy_gap.sort_index(inplace=True)\n",
    "xy_known.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally estimate MAPEs of all forecasts. We need to mention that in the transformed data set returned by forecast() method the column with forecast is called _automl_target_column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the actual comparisons of MAPES.\n",
    "print(\"MAPE of data set with no gap: {}\".format(MAPE(X_test_trimmed[TARGET_COLUMN_NAME].values,\n",
    "                                                     xy_nogap_trimmed['_automl_target_col'].values)))\n",
    "print(\"MAPE of data set with gap: {}\".format(MAPE(X_test_trimmed[TARGET_COLUMN_NAME].values,\n",
    "                                                  xy_gap['_automl_target_col'].values)))\n",
    "print(\"MAPE of data set with previous y knowlege: {}\".format(MAPE(X_test_trimmed[TARGET_COLUMN_NAME].values,\n",
    "                                                                  xy_known['_automl_target_col'].values)))\n",
    "print(\"MAPE of forecast with target date: {}\".format(MAPE(X_test_trimmed[TARGET_COLUMN_NAME].values,\n",
    "                                                          xy_dest_trimmed['_automl_target_col'].values)))\n",
    "print(\"MAPE of forecast with ragged data: {}\".format(MAPE(np.repeat(np.array([16., 17., 18., 19.]), 2),\n",
    "                                                          xy_ragged['_automl_target_col'].values)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nativetest",
   "language": "python",
   "name": "nativetest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}