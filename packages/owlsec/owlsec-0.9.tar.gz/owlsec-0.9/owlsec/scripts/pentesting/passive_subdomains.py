import requests
import re
from urllib.parse import urlparse, parse_qsl
import tldextract

def get_subdomains_and_urls(domain):
    """
    Retrieves subdomains and interesting URLs for a given domain using Web Archive.

    :param domain: Target domain to retrieve subdomains and URLs for.
    :type domain: str
    :returns: Tuple containing a list of subdomains and a list of URLs.
    :rtype: tuple
    """

    def fetch_archive(domain):
        archive_url = f"https://web.archive.org/cdx/search/cdx?url=*.{domain}&output=xml&fl=original&collapse=urlkey"
        try:
            response = requests.get(archive_url)
            response.raise_for_status()
            return response.text, None
        except requests.RequestException as e:
            return None, str(e)

    def extract_interesting_urls(response):
        url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')
        filter_pattern = re.compile(r'/[^/?]+?\?[^=]+=.')
        urls = url_pattern.findall(response)
        filtered_urls = []
        param_sets = []
        for url in urls:
            if filter_pattern.search(url):
                filtered_urls.append(url)
                params = urlparse(url).query
                param_names = sorted(key for key, value in parse_qsl(params))
                param_sets.append(set(param_names))
        unique_urls = [filtered_urls[0]]
        unique_params = {frozenset(param_sets[0])}
        for i in range(1, len(filtered_urls)):
            if param_sets[i] not in unique_params:
                unique_urls.append(filtered_urls[i])
                unique_params.add(frozenset(param_sets[i]))
        return unique_urls 

    extracted_domain = tldextract.extract(domain)
    stripped_domain = f"{extracted_domain.domain}.{extracted_domain.suffix}"
    response, error = fetch_archive(stripped_domain)
    if error or not response:
        return [], []

    urls = response.strip().split("\n")
    urls.append(stripped_domain)
    subdomains = set()
    for url in urls:
        extracted = tldextract.extract(url)
        full_subdomain = f"{extracted.subdomain}.{extracted.domain}.{extracted.suffix}".strip('.')
        if extracted.domain == extracted_domain.domain and extracted.suffix == extracted_domain.suffix:
            subdomains.add(full_subdomain)
    root_domain = f"{extracted_domain.domain}.{extracted_domain.suffix}"
    www_domain = f"www.{root_domain}"
    if root_domain in subdomains and www_domain in subdomains:
        subdomains.remove(www_domain)
    sorted_subdomains = sorted(list(subdomains), key=len)
    if not sorted_subdomains:
        sorted_subdomains = [stripped_domain]
    interesting_urls = extract_interesting_urls(response)
    return sorted_subdomains, interesting_urls

if __name__ == "__main__":
    domain = "google.com"    
    sorted_subdomains, interesting_urls = get_subdomains_and_urls(domain)
    for interesting_url in interesting_urls:
        print(f"{interesting_url}")
    for subdomain_found in sorted_subdomains:
        print(f"{subdomain_found}")